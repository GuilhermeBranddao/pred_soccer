{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.chdir(\"..\")\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 13:21:33,203 | main | INFO | logs | <module> | Logger configurado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "from modelagem.utils.logs import logger\n",
    "from modelagem.train import model_trainer\n",
    "\n",
    "# Ler 500 colunas no pandas \n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelagem.train.model_trainer import load_model, load_json\n",
    "from modelagem.settings.config import Settings\n",
    "config = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Testando modelo na pasta: 20250416_131211_first\n",
      "[{'result': 'A', 'recall': 38.0, 'f1': 40.0}, {'result': 'D', 'recall': 32.0, 'f1': 36.0}, {'result': 'H', 'recall': 49.0, 'f1': 45.0}]\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ§ª Testando modelo na pasta: 20250416_131321_basic\n",
      "[{'result': 'A', 'recall': 2.0, 'f1': 4.0}, {'result': 'D', 'recall': 53.0, 'f1': 44.0}, {'result': 'H', 'recall': 49.0, 'f1': 43.0}]\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“Š Resultados:\n",
      "                   model  accuracy  f1_score  recall\n",
      "0  20250416_131211_first  0.417949       NaN     NaN\n",
      "1  20250416_131321_basic  0.382051       NaN     NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pyprojects\\pred_soccer\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\pyprojects\\pred_soccer\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from modelagem.utils.metrics import (\n",
    "    get_precision, \n",
    "    get_multiclass_confusion_counts, \n",
    "    calc_recall, \n",
    "    calc_f1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def calc_metrics_multclass(dict_confusion_counts, result_mapping):\n",
    "\n",
    "    [dict_confusion_counts.get(key) for key in dict_confusion_counts.keys()]\n",
    "    result_id_mapping = {result_mapping.get(k):k for k in result_mapping}\n",
    "\n",
    "    list_metrics_multclass = []\n",
    "    for key in dict_confusion_counts.keys():\n",
    "        key_confusion = dict_confusion_counts.get(key)\n",
    "        recall = calc_recall(tp=key_confusion.get('tp'),\n",
    "                                            fn=key_confusion.get('fn'))\n",
    "\n",
    "        f1 = calc_f1(precision=precision, recall=recall)\n",
    "        \n",
    "        # print(result_id_mapping.get(key), recall, f1)\n",
    "        list_metrics_multclass.append({\n",
    "                \"result\":result_id_mapping.get(key),\n",
    "                \"recall\": recall*100,\n",
    "                \"f1\": f1*100,\n",
    "            })\n",
    "    return list_metrics_multclass\n",
    "\n",
    "path_models = Path(\"database/models\")\n",
    "results = []\n",
    "\n",
    "if not path_models.exists():\n",
    "    print(\"âŒ DiretÃ³rio 'database/models' nÃ£o encontrado.\")\n",
    "else:\n",
    "    for folders in os.listdir(path_models):\n",
    "        path_files = path_models / folders\n",
    "        list_files = os.listdir(path_files)\n",
    "\n",
    "        if any(\".pkl\" in file for file in list_files):\n",
    "            try:\n",
    "                print(f\"\\nðŸ§ª Testando modelo na pasta: {folders}\")\n",
    "                path_model = path_files / \"logistic_regression_model.pkl\"\n",
    "                path_mapping_feature_order = path_files / \"mapping\" / \"feature_order.json\"\n",
    "                path_mapping_result_mapping = path_files / \"mapping\" / \"result_mapping.json\"\n",
    "\n",
    "                path_feature = path_files / \"features\" / \"ft_df.csv\"\n",
    "\n",
    "                if not path_model.exists() or not path_feature.exists() or not path_mapping_feature_order.exists():\n",
    "                    print(\"âŒ Arquivos necessÃ¡rios nÃ£o encontrados, pulando...\")\n",
    "                    continue\n",
    "\n",
    "                # Carrega os dados\n",
    "                df = pd.read_csv(path_feature)\n",
    "                df_validate = df[df[\"season\"] >= 2024]\n",
    "                target_column = 'result_encoded'\n",
    "                x_validate = df_validate.drop(columns=target_column)\n",
    "                y_validate = df_validate[target_column]\n",
    "\n",
    "                # Carrega o modelo\n",
    "                with open(path_model, 'rb') as f:\n",
    "                    model = pickle.load(f)\n",
    "\n",
    "                # Carrega a ordem das features\n",
    "                result_mapping = load_json(path_mapping_result_mapping)\n",
    "                feature_order = load_json(path_mapping_feature_order)\n",
    "\n",
    "                # Prepara os dados\n",
    "                X_test = x_validate[feature_order]\n",
    "                scaler = StandardScaler()\n",
    "                X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "                # Avalia\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                # acc = accuracy_score(y_validate, y_pred)\n",
    "                # f1 = f1_score(y_validate, y_pred, average='weighted')\n",
    "\n",
    "                precision = get_precision(y_pred=y_pred, \n",
    "                                          y_true=y_validate)\n",
    "                \n",
    "                dict_confusion_counts = get_multiclass_confusion_counts(y_pred=y_pred, \n",
    "                                          y_true=y_validate)\n",
    "                \n",
    "\n",
    "                recall = calc_recall(tp=dict_confusion_counts.get('tp'),\n",
    "                                    fn=dict_confusion_counts.get('fn'))\n",
    "\n",
    "                f1 = calc_f1(precision=precision, recall=recall)\n",
    "\n",
    "                list_metrics_multclass = calc_metrics_multclass(dict_confusion_counts, result_mapping)\n",
    "\n",
    "                print(list_metrics_multclass)\n",
    "\n",
    "                print('---'*20)\n",
    "                results.append({\n",
    "                    \"model\": folders,\n",
    "                    \"accuracy\": precision,\n",
    "                    \"f1_score\": f1,\n",
    "                    \"recall\":recall,\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Erro ao testar o modelo {folders}: {e}\")\n",
    "\n",
    "    if results:\n",
    "        df_results = pd.DataFrame(results)\n",
    "        print(\"\\nðŸ“Š Resultados:\")\n",
    "        print(df_results.sort_values(by=\"f1_score\", ascending=False))\n",
    "    else:\n",
    "        print(\"âš ï¸ Nenhum modelo vÃ¡lido foi testado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'result': 'A', 'recall': 2.0, 'f1': 4.0},\n",
       " {'result': 'D', 'recall': 53.0, 'f1': 44.0},\n",
       " {'result': 'H', 'recall': 49.0, 'f1': 43.0}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_metrics_multclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                   model  accuracy  f1_score  \\\n",
    "# 0  20250416_131211_first  0.417949  0.424845   \n",
    "# 1  20250416_131321_basic  0.382051  0.346784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'database/models\\\\20250416_113549_teste_inplementacao'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_model = list_path_models_ok[0]\n",
    "\n",
    "path_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\pyprojects\\pred_soccer\\database\\models\\20250416_113609_teste_inplementacao\\features\\ft_df.csv\")\n",
    "\n",
    "df_validate = df[df[\"season\"] >= 2024]\n",
    "\n",
    "target_column = 'result_encoded'\n",
    "\n",
    "x_validate = df_validate.drop(columns=target_column)\n",
    "y_validate = df_validate[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
